<!DOCTYPE html>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title></title>
	<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Awesome Papers: 2017-01-3</title>
<meta property="og:title" content="Awesome Papers: 2017-01-3" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Some papers about reinforcement learning" />
<meta property="og:description" content="Some papers about reinforcement learning" />
<link rel="canonical" href="http://localhost:4000/weekly/2017/02/06/weekly201701-3.html" />
<meta property="og:url" content="http://localhost:4000/weekly/2017/02/06/weekly201701-3.html" />
<meta property="og:image" content="http://localhost:4000/assets/images/weekly201701-3.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-02-06T00:00:00+08:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"name":null,"description":"Some papers about reinforcement learning","author":null,"@type":"BlogPosting","url":"http://localhost:4000/weekly/2017/02/06/weekly201701-3.html","publisher":null,"image":"http://localhost:4000/assets/images/weekly201701-3.jpg","headline":"Awesome Papers: 2017-01-3","dateModified":"2017-02-06T00:00:00+08:00","datePublished":"2017-02-06T00:00:00+08:00","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/weekly/2017/02/06/weekly201701-3.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
  
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="http://localhost:4000" class="logo"><strong>Learning Group supported by Yanghe Feng</strong> <span></span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</header>



<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="/" class="button special fit">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
        
            <li><a href="/paper/" class="button fit">Lastest Studies</a></li>
        
            <li><a href="/weekly/" class="button fit">Awesome Papers Weekly</a></li>
        
            <li><a href="/study/" class="button fit">Intensive Studies</a></li>
        
        
			
        
			
				<ul class="actions vertical">
				<li><a href="/cv.html" class="button fit">About us</a></li>
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        

	    
<!--
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
-->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Awesome Papers: 2017-01-3</h1>
		</header>
		<span class="image main"><img src="/assets/images/weekly201701-3.jpg" alt="" /></span>
		<p><h3 id="online-reinforcement-learning-for-real-time-exploration-in-continuous-state-and-action-markov-decision-processes">Online Reinforcement Learning for Real-Time Exploration in Continuous State and Action Markov Decision Processes</h3>

<p><em>Ludovic Hofer, Hugo Gimbert</em></p>

<p>This paper presents a new method to learn online policies in continuous state, continuous action, model-free Markov decision processes, with two properties that are crucial for practical applications.<!--excerpt-->First, the policies are implementable with a very low computational cost: once the policy is computed,the action corresponding to a given state is obtained in logarithmic time with respect to the number of samples used. Second, our method is versatile: it does not rely on any a priori knowledge of the structure of optimal policies. We build upon the Fitted Q-iteration algorithm which represents the $Q$-value as the average of several regression trees. Our algorithm, the Fitted Policy Forest algorithm (FPF), computes a regression forest representing the Q-value 
and transforms it into a single tree representing the policy, while keeping control on the size of the policy using resampling and leaf merging. We introduce an adaptation of Multi-Resolution Exploration (MRE) which is 
particularly suited to FPF. We assess the performance of FPF on three classical benchmarks for reinforcement learning: the “Inverted Pendulum”, the “Double Integrator” and “Car on the Hill” and show that FPF equals or outperforms other algorithms, although these algorithms rely on the use of particular representations of the policies, especially chosen in order to fit each of the 
three problems. Finally, we exhibit that the combination of FPF and MRE allows to find nearly optimal solutions in problems where $\epsilon$-greedy approaches would fail.</p>

<hr />

<h3 id="deepmind-lab">DeepMind Lab</h3>

<p><em>Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K"uttler, Andrew Lefrancq, Simon Green, V'ictor Vald'es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis,Shane Legg and Stig Petersen</em></p>

<p>DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems.DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.</p>

<hr />

<h3 id="multiple-instance-learning-a-survey-of-problem-characteristics-and-applications">Multiple Instance Learning: A Survey of Problem Characteristics and Applications</h3>

<p><em>Marc-Andr'e Carbonneau, Veronika Cheplygina, Eric Granger and Ghyslain Gagnon</em></p>

<p>Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In 
this paper, MIL problem characteristics are grouped into four broad categories:the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally,experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research.</p>

<hr />

<h3 id="design-and-development-of-bayes-minimax-linear-classification-systems">Design and Development of Bayes’ Minimax Linear Classification Systems</h3>

<p><em>Denise M. Reeves</em></p>

<p>This paper considers the design and development of Bayes’ minimax, linear classification systems using linear discriminant functions that are Bayes’ equalizer rules. Bayes’ equalizer rules divide two-class feature spaces into decision regions that have equal classification errors. I will formulate the problem of learning unknown linear discriminant functions from data as a locus problem, thereby formulating geometric locus methods within a statistical framework. Solving locus problems involves finding the equation of a curve or surface defined by a given property, and finding the graph or locus of a given equation. I will devise a system of locus equations that determines Bayes’equalizer rules which is based on a variant of the inequality constrained optimization problem for linear kernel support vector machines. Thereby, I will define a class of learning machines which are fundamental building blocks for Bayes’ minimax pattern recognition systems.</p>

<hr />

<h3 id="the-predictron-end-to-end-learning-and-planning">The Predictron: End-To-End Learning and Planning</h3>

<p><em>David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris</em></p>

<p>One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model,represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally 
generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.</p>

<hr />

<h3 id="attend-adapt-and-transfer-attentive-deep-architecture-for-adaptive-transfer-from-multiple-source-tasks">Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from Multiple Source Tasks</h3>

<p><em>Janarthanan Rajendran, Aravind S. Lakshminarayanan, Mitesh M. Khapra, P Prasanna, Balaraman Ravindran</em></p>

<p>Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend, Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.</p>

<hr />

<h3 id="non-deterministic-policy-improvement-stabilizes-approximated-reinforcement-learning">Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning</h3>

<p><em>Wendelin B"ohmer and Rong Guo and Klaus Obermayer</em></p>

<p>This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements’ stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.</p>

<hr />

<h3 id="how-to-train-your-deep-neural-network-with-dictionary-learning">How to Train Your Deep Neural Network with Dictionary Learning</h3>

<p><em>Vanika Singhal, Shikha Singh and Angshul Majumdar</em></p>

<p>Currently there are two predominant ways to train deep neural networks. The first one uses restricted Boltzmann machine (RBM) and the second one autoencoders. RBMs are stacked in layers to form deep belief network (DBN); the 
final representation layer is attached to the target to complete the deep neural network. Autoencoders are nested one inside the other to form stacked autoencoders; once the stcaked autoencoder is learnt the decoder portion is 
detached and the target attached to the deepest layer of the encoder to form the deep neural network. This work proposes a new approach to train deep neural networks using dictionary learning as the basic building block; the idea is to use the features from the shallower layer as inputs for training the next deeper layer. One can use any type of dictionary learning (unsupervised,supervised, discriminative etc.) as basic units till the pre-final layer. In the final layer one needs to use the label consistent dictionary learning formulation for classification. We compare our proposed framework with existing state-of-the-art deep learning techniques on benchmark problems; we are always within the top 10 results. In actual problems of age and gender classification,we are better than the best known techniques.</p>

<hr />

<h3 id="a-note-on-the-function-approximation-error-bound-for-risk-sensitive--reinforcement-learning">A note on the function approximation error bound for risk-sensitive  reinforcement learning</h3>

<p><em>Prasenjit Karmakar, Shalabh Bhatnagar</em></p>

<p>In this paper we improve the existing function approximation error bound for the policy evaluation algorithm when the aim is to find the risk-sensitive cost 
represented using exponential utility.</p>

<hr />

<h3 id="deep-learning-and-its-applications-to-machine-health-monitoring-a-survey">Deep Learning and Its Applications to Machine Health Monitoring: A Survey</h3>

<p><em>Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang and Robert X. Gao</em> 
Categories: cs.LG stat.ML</p>

<p>Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data.The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in 
machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines 
(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).Finally, some new trends of DL-based machine health monitoring methods are discussed.</p>

<hr />

<h3 id="adaptive-lambda-least-squares-temporal-difference-learning">Adaptive Lambda Least-Squares Temporal Difference Learning</h3>

<p><em>Timothy A. Mann and Hugo Penedones and Shie Mannor and Todd Hester</em></p>

<p>Temporal Difference learning or TD(λ) is a fundamental algorithm in the field of reinforcement learning. However, setting TD’s λ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the λ selection problem as a bias-variance trade-off where the solution is the value of λ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of λ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune λ and apply function optimization methods to efficiently search the space of λ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na"{i}ve LOTO-CV implementation while achieving similar performance.</p>

</p>
	</div>
</section>

</div>

    <!-- Contact -->
<section id="contact">
	<div class="inner">
		<section>
			<form action="https://formspree.io/fengyanghe@gmail.com" method="POST">
				<div class="field half first">
					<label for="name">Name</label>
					<input type="text" name="name" id="name" />
				</div>
				<div class="field half">
					<label for="email">Email</label>
					<input type="text" name="_replyto" id="email" />
				</div>
				<div class="field">
					<label for="message">Message</label>
					<textarea name="message" id="message" rows="6"></textarea>
				</div>
				<ul class="actions">
					<li><input type="submit" value="Send Message" class="special" /></li>
					<li><input type="reset" value="Clear" /></li>
				</ul>
			</form>
		</section>
		<section class="split">
			<section>
				<div class="contact-method">
					<span class="icon alt fa-envelope"></span>
					<h3>Email</h3>
					<a href="#">fengyanghe@gmail.com</a>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-phone"></span>
					<h3>Phone</h3>
					<span>07318457661</span>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-home"></span>
					<h3>Address</h3>
					<span>National University of Defense Tecnology<br />
					Changsha, Hunan 410073<br />
					China</span>
				</div>
			</section>
		</section>
	</div>
</section>

<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				
				
				
				
				
				
				<a href="https://www.github.com/fengyanghe" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a>
				
				
				
			</ul>
			<ul class="copyright">
				<li>&copy; Learning Group supported by Yanghe Feng </li>
				<li>Design: <a href="https://fengyanghe.github.io" target="_blank">Yanghe Feng</a></li>
				<li>Jekyll integration: <a href="https://fengyanghe.github.io" target="_blank">Yanghe Feng</a></li>
				
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/assets/js/main.js"></script>


</body>

</html>
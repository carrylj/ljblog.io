<!DOCTYPE html>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title></title>
	<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Awesome Papers: 2017-02-1</title>
<meta property="og:title" content="Awesome Papers: 2017-02-1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Some papers about reinforcement learning" />
<meta property="og:description" content="Some papers about reinforcement learning" />
<link rel="canonical" href="http://localhost:4000/weekly/2017/02/06/weekly201702-1.html" />
<meta property="og:url" content="http://localhost:4000/weekly/2017/02/06/weekly201702-1.html" />
<meta property="og:image" content="http://localhost:4000/assets/images/weekly201702-1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-02-06T00:00:00+08:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"name":null,"description":"Some papers about reinforcement learning","author":null,"@type":"BlogPosting","url":"http://localhost:4000/weekly/2017/02/06/weekly201702-1.html","publisher":null,"image":"http://localhost:4000/assets/images/weekly201702-1.jpg","headline":"Awesome Papers: 2017-02-1","dateModified":"2017-02-06T00:00:00+08:00","datePublished":"2017-02-06T00:00:00+08:00","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/weekly/2017/02/06/weekly201702-1.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
  
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="http://localhost:4000" class="logo"><strong>Learning Group supported by Yanghe Feng</strong> <span></span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</header>



<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="/" class="button special fit">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
        
            <li><a href="/paper/" class="button fit">Lastest Studies</a></li>
        
            <li><a href="/weekly/" class="button fit">Awesome Papers Weekly</a></li>
        
            <li><a href="/study/" class="button fit">Intensive Studies</a></li>
        
        
			
        
			
				<ul class="actions vertical">
				<li><a href="/cv.html" class="button fit">About us</a></li>
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        
			
        

	    
<!--
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
-->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Awesome Papers: 2017-02-1</h1>
		</header>
		<span class="image main"><img src="/assets/images/weekly201702-1.jpg" alt="" /></span>
		<p><h3 id="on-the-construction-of-extreme-learning-machine-for-online-and-offline-one-class-classification---an-expanded-toolbox">On The Construction of Extreme Learning Machine for Online and Offline One-Class Classification - An Expanded Toolbox</h3>

<p><em>Chandan Gautam, Aruna Tiwari and Qian Leng</em></p>

<h2 id="one-class-classification-occ-has-been-prime-concern-for-researchers-and-effectively-employed-in-various-disciplines--but-traditional-methods-based-one-class-classifiers-are-very-time-consuming-due-to-its-iterative-process-and-various-parameters-tuning-in-this-paper-we-present-six-occ-methods-based-on-extreme-learning-machine-elm-and-online-sequential-elm-oselm-our-proposed-classifiers-mainly-lie-in-two-categories-reconstruction-based-and-boundary-based-which-supports-both-types-of-learning-viz-online-and-offline-learning-out-of-various-proposed-methods-four-are-offline-and-remaining-two-are-online-methods-out-of-four-offline-methods-two-methods-perform-random-feature-mapping-and-two-methods-perform-kernel-feature-mapping-kernel-feature-mapping-based-approaches-have-been-tested-with-rbf-kernel-and-online-version-of-one-class-classifiers-are-tested-with-both-types-of-nodes-viz-additive-and-rbf-it-is-well-known-fact-that-threshold-decision-is-a-crucial-factor-in-case-of-occ-so-three-different-threshold-deciding-criteria-have-been-employed-so-far-and-analyses-the-effectiveness-of-one-threshold-deciding-criteria-over-another-further-these-methods-are-tested-on-two-artificial-datasets-to-check-there-boundary-construction-capability-and-on-eight-benchmark-datasets-from-different-discipline-to-evaluate-the-performance-of-the-classifiers-our-proposed-classifiers-exhibit-better-performance-compared-to-ten-traditional-one-class-classifiers-and-elm-based-two-one-class-classifiers-through-proposed-one-class-classifiers-we-intend-to-expand-the-functionality-of-the-most-used-toolbox-for-occ-ie-dd-toolbox-all-of-our-methods-are-totally-compatible-with-all-the-present-features-of-the-toolbox">One-Class Classification (OCC) has been prime concern for researchers and effectively employed in various disciplines. <!--excerpt--> But, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. In this paper, we present six OCC methods based on extreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed classifiers mainly lie in two categories: reconstruction based and boundary based, which supports both types of learning viz., online and offline learning. Out of various proposed methods, four are offline and remaining two are online methods. Out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. Kernel feature mapping based approaches have been tested with RBF kernel and online version of one-class classifiers are tested with both types of nodes viz., additive and RBF. It is well known fact that threshold decision is a crucial factor in case of OCC, so, three different threshold deciding criteria have been employed so far and analyses the effectiveness of one threshold deciding criteria over another. Further, these methods are tested on two artificial datasets to check there boundary construction capability and on eight benchmark datasets from different discipline to evaluate the performance of the classifiers. Our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and ELM based two one-class classifiers. Through proposed one-class classifiers, we intend to expand the functionality of the most used toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with all the present features of the toolbox.</h2>

<h3 id="adversarial-variational-bayes-unifying-variational-autoencoders-and-generative-adversarial-networks">Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks</h3>

<p><em>Lars Mescheder, Sebastian Nowozin and Andreas Geiger</em></p>

<p>Variational Autoencoders (VAEs) are expressive latent variable models that 
can be used to learn complex probability distributions from training data. 
However, the quality of the resulting model crucially relies on the 
expressiveness of the inference model used during training. We introduce 
Adversarial Variational Bayes (AVB), a technique for training Variational 
Autoencoders with arbitrarily expressive inference models. We achieve this by 
introducing an auxiliary discriminative network that allows to rephrase the 
maximum-likelihood-problem as a two-player game, hence establishing a 
principled connection between VAEs and Generative Adversarial Networks (GANs). 
We show that in the nonparametric limit our method yields an exact 
maximum-likelihood assignment for the parameters of the generative model, as 
well as the exact posterior distribution over the latent variables given an 
observation. Contrary to competing approaches which combine VAEs with GANs, our 
approach has a clear theoretical justification, retains most advantages of 
standard Variational Autoencoders and is easy to implement.</p>

<hr />

<h3 id="agent-agnostic-human-in-the-loop-reinforcement-learning">Agent-Agnostic Human-in-the-Loop Reinforcement Learning</h3>

<p><em>David Abel, John Salvatier, Andreas Stuhlm"uller, Owain Evans</em></p>

<p>Providing Reinforcement Learning agents with expert advice can dramatically 
improve various aspects of learning. Prior work has developed teaching 
protocols that enable agents to learn efficiently in complex environments; many 
of these methods tailor the teacher’s guidance to agents with a particular 
representation or underlying learning scheme, offering effective but 
specialized teaching procedures. In this work, we explore protocol programs, an 
agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is 
to incorporate the beneficial properties of a human teacher into Reinforcement 
Learning without making strong assumptions about the inner workings of the 
agent. We show how to represent existing approaches such as action pruning, 
reward shaping, and training in simulation as special cases of our schema and 
conduct preliminary experiments on simple domains.</p>

<hr />

<h3 id="near-optimal-behavior-via-approximate-state-abstraction">Near Optimal Behavior via Approximate State Abstraction</h3>

<p><em>David Abel, D. Ellis Hershkowitz, Michael L. Littman</em></p>

<p>The combinatorial explosion that plagues planning and reinforcement learning 
(RL) algorithms can be moderated using state abstraction. Prohibitively large 
task representations can be condensed such that essential information is 
preserved, and consequently, solutions are tractably computable. However, exact 
abstractions, which treat only fully-identical situations as equivalent, fail 
to present opportunities for abstraction in environments where no two 
situations are exactly alike. In this work, we investigate approximate state 
abstractions, which treat nearly-identical situations as equivalent. We present 
theoretical guarantees of the quality of behaviors derived from four types of 
approximate abstractions. Additionally, we empirically demonstrate that 
approximate abstractions lead to reduction in task complexity and bounded loss 
of optimality of behavior in a variety of environments.</p>

<hr />

<h3 id="vulnerability-of-deep-reinforcement-learning-to-policy-induction-attacks">Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks</h3>

<p><em>Vahid Behzadan and Arslan Munir</em></p>

<p>Deep learning classifiers are known to be inherently vulnerable to 
manipulation by intentionally perturbed inputs, named adversarial examples. In 
this work, we establish that reinforcement learning techniques based on Deep 
Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and 
verify the transferability of adversarial examples across different DQN models. 
Furthermore, we present a novel class of attacks based on this vulnerability 
that enable policy manipulation and induction in the learning process of DQNs. 
We propose an attack mechanism that exploits the transferability of adversarial 
examples to implement policy induction attacks on DQNs, and demonstrate its 
efficacy and impact through experimental study of a game-learning scenario.</p>

<hr />

<h3 id="a-threshold-based-scheme-for-reinforcement-learning-in-neural-networks">A Threshold-based Scheme for Reinforcement Learning in Neural Networks</h3>

<p><em>Thomas H. Ward</em></p>

<p>A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.</p>

<hr />

<h3 id="building-machines-that-learn-and-think-like-people">Building Machines That Learn and Think Like People</h3>

<p><em>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman</em></p>

<p>Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.</p>

<hr />

<h3 id="the-predictron-end-to-end-learning-and-planning">The Predictron: End-To-End Learning and Planning</h3>

<p><em>David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris</em></p>

<p>One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.</p>

<hr />

<h3 id="learning-what-to-look-in-chest-x-rays-with-a-recurrent-visual-attention-model">Learning what to look in chest X-rays with a recurrent visual attention model</h3>

<p><em>Petros-Pavlos Ypsilantis and Giovanni Montana</em></p>

<p>X-rays are commonly performed imaging tests that use small amounts of 
radiation to produce pictures of the organs, tissues, and bones of the body. 
X-rays of the chest are used to detect abnormalities or diseases of the 
airways, blood vessels, bones, heart, and lungs. In this work we present a 
stochastic attention-based model that is capable of learning what regions 
within a chest X-ray scan should be visually explored in order to conclude that 
the scan contains a specific radiological abnormality. The proposed model is a 
recurrent neural network (RNN) that learns to sequentially sample the entire 
X-ray and focus only on informative areas that are likely to contain the 
relevant information. We report on experiments carried out with more than 
100,000 X-rays containing enlarged hearts or medical devices. The model has 
been trained using reinforcement learning methods to learn task-specific 
policies.</p>

<hr />

<h3 id="learning-to-reinforcement-learn">Learning to reinforcement learn</h3>

<p><em>Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,Charles Blundell, Dharshan Kumaran, Matt Botvinick</em></p>

<p>In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.</p>
</p>
	</div>
</section>

</div>

    <!-- Contact -->
<section id="contact">
	<div class="inner">
		<section>
			<form action="https://formspree.io/fengyanghe@gmail.com" method="POST">
				<div class="field half first">
					<label for="name">Name</label>
					<input type="text" name="name" id="name" />
				</div>
				<div class="field half">
					<label for="email">Email</label>
					<input type="text" name="_replyto" id="email" />
				</div>
				<div class="field">
					<label for="message">Message</label>
					<textarea name="message" id="message" rows="6"></textarea>
				</div>
				<ul class="actions">
					<li><input type="submit" value="Send Message" class="special" /></li>
					<li><input type="reset" value="Clear" /></li>
				</ul>
			</form>
		</section>
		<section class="split">
			<section>
				<div class="contact-method">
					<span class="icon alt fa-envelope"></span>
					<h3>Email</h3>
					<a href="#">fengyanghe@gmail.com</a>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-phone"></span>
					<h3>Phone</h3>
					<span>07318457661</span>
				</div>
			</section>
			<section>
				<div class="contact-method">
					<span class="icon alt fa-home"></span>
					<h3>Address</h3>
					<span>National University of Defense Tecnology<br />
					Changsha, Hunan 410073<br />
					China</span>
				</div>
			</section>
		</section>
	</div>
</section>

<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				
				
				
				
				
				
				<a href="https://www.github.com/fengyanghe" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a>
				
				
				
			</ul>
			<ul class="copyright">
				<li>&copy; Learning Group supported by Yanghe Feng </li>
				<li>Design: <a href="https://fengyanghe.github.io" target="_blank">Yanghe Feng</a></li>
				<li>Jekyll integration: <a href="https://fengyanghe.github.io" target="_blank">Yanghe Feng</a></li>
				
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/assets/js/main.js"></script>


</body>

</html>